ğŸ§  What is KNN?

    -K-Nearest Neighbors is a supervised, distance-based machine learning algorithm.

For a new data point:

    -Compute distance to all training points
    -Select the K closest neighbors
    -Perform majority voting
    -Assign the most common class

ğŸ“Š Dataset Used

    -Iris Dataset from sklearn.datasets:
    -150 samples
    -3 classes
    -4 features: Sepal length, Sepal width, Petal length, Petal width

âš™ï¸ Pre-processing
ğŸ”¹ Feature Scaling
    -StandardScaler is used before applying KNN.
    Why?
    KNN relies on distance calculations. If features have different ranges, larger-scale features dominate.


âœ… Implementation 1: Using Scikit-Learn

                    Steps:

                    - Load dataset
                    - Convert to DataFrame
                    - Train-test split
                    - Apply StandardScaler
                    - Train KNN classifier
                    - Predict on test set
                    - Evaluate using:
                    - Accuracy
                    - Precision
                    - Confusion Matrix

âœ… Implementation 2: KNN From Scratch

                      Implemented manually:

                    - Euclidean distance function
                    - Loop over test points
                    - Compute distance to all training points
                    - Sort neighbors
                    - Select top K
                    - Majority voting using Counter
                    - Evaluate accuracy
                    - Try multiple values of K

ğŸ§ª Testing Different K Values

    Odd values of K were tested to avoid ties: K = 1, 3, 5, 7, 9


All achieved high accuracy due to: Clean dataset, Well separated classes, Proper scaling

ğŸ“ˆ Evaluation Metrics: Accuracy Score, Precision Score (weighted), Confusion Matrix

ğŸ§¾ Key Learnings

    âœ” How distance-based algorithms work
    âœ” Why scaling is critical
    âœ” How majority voting is implemented
    âœ” How K affects model behavior
    âœ” Difference between library vs scratch implementation
    âœ” Computational cost of KNN

    âš ï¸ Time Complexity

        - For each test point:
        - Distance calculation â†’ O(n Ã— d)
        - Sorting â†’ O(n log n)
        - This makes KNN expensive for very large datasets.